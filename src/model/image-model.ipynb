{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Failed to auto-detect ffmpeg and ffprobe executable.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import ffmpegio\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import torch\n",
    "from transformers import MobileViTFeatureExtractor, MobileViTForSemanticSegmentation\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def convert_number(num: str) -> int:\n",
    "    \"\"\"\n",
    "    Убирает суффиксы К и М из чисел\n",
    "\n",
    "    :param num: число, которое нужно преобразовать;\n",
    "    :return: преобразованное число\n",
    "    \"\"\"\n",
    "    if 'K' in num:\n",
    "        return int(float(num[:-1]) * 1_000)\n",
    "    elif 'M' in num:\n",
    "        return int(float(num[:-1]) * 1_000_000)\n",
    "    else:\n",
    "        return int(num)\n",
    "\n",
    "\n",
    "renderer = 'svg'  # Как будет сохраняться график в ноутбуке\n",
    "ffmpegio.set_path(r\"..\\..\\ffmpeg-master-latest-win64-gpl\\bin\")\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA\n",
    "\n",
    "\n",
    "## Распределение отношения лайков к просмотрам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg class=\"main-svg\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"700\" height=\"512\" style=\"\" viewBox=\"0 0 700 512\"><rect x=\"0\" y=\"0\" width=\"700\" height=\"512\" style=\"fill: rgb(255, 255, 255); fill-opacity: 1;\"/><defs id=\"defs-c04c10\"><g class=\"clips\"><clipPath id=\"clipc04c10xyplot\" class=\"plotclip\"><rect width=\"538\" height=\"332\"/></clipPath><clipPath class=\"axesclip\" id=\"clipc04c10x\"><rect x=\"80\" y=\"0\" width=\"538\" height=\"512\"/></clipPath><clipPath class=\"axesclip\" id=\"clipc04c10y\"><rect x=\"0\" y=\"100\" width=\"700\" height=\"332\"/></clipPath><clipPath class=\"axesclip\" id=\"clipc04c10xy\"><rect x=\"80\" y=\"100\" width=\"538\" height=\"332\"/></clipPath></g><g class=\"gradients\"/><g class=\"patterns\"/></defs><g class=\"bglayer\"><rect class=\"bg\" x=\"80\" y=\"100\" width=\"538\" height=\"332\" style=\"fill: rgb(229, 236, 246); fill-opacity: 1; stroke-width: 0;\"/></g><g class=\"layer-below\"><g class=\"imagelayer\"/><g class=\"shapelayer\"/></g><g class=\"cartesianlayer\"><g class=\"subplot xy\"><g class=\"layer-subplot\"><g class=\"shapelayer\"/><g class=\"imagelayer\"/></g><g class=\"minor-gridlayer\"><g class=\"x\"/><g class=\"y\"/></g><g class=\"gridlayer\"><g class=\"x\"/><g class=\"y\"><path class=\"ygrid crisp\" transform=\"translate(0,367.63)\" d=\"M80,0h538\" style=\"stroke: rgb(255, 255, 255); stroke-opacity: 1; stroke-width: 1px;\"/><path class=\"ygrid crisp\" transform=\"translate(0,303.27)\" d=\"M80,0h538\" style=\"stroke: rgb(255, 255, 255); stroke-opacity: 1; stroke-width: 1px;\"/><path class=\"ygrid crisp\" transform=\"translate(0,238.9)\" d=\"M80,0h538\" style=\"stroke: rgb(255, 255, 255); stroke-opacity: 1; stroke-width: 1px;\"/><path class=\"ygrid crisp\" transform=\"translate(0,174.53)\" d=\"M80,0h538\" style=\"stroke: rgb(255, 255, 255); stroke-opacity: 1; stroke-width: 1px;\"/><path class=\"ygrid crisp\" transform=\"translate(0,110.16)\" d=\"M80,0h538\" style=\"stroke: rgb(255, 255, 255); stroke-opacity: 1; stroke-width: 1px;\"/></g></g><g class=\"zerolinelayer\"><path class=\"yzl zl crisp\" transform=\"translate(0,432)\" d=\"M80,0h538\" style=\"stroke: rgb(255, 255, 255); stroke-opacity: 1; stroke-width: 2px;\"/></g><path class=\"xlines-below\"/><path class=\"ylines-below\"/><g class=\"overlines-below\"/><g class=\"xaxislayer-below\"/><g class=\"yaxislayer-below\"/><g class=\"overaxes-below\"/><g class=\"plot\" transform=\"translate(80,100)\" clip-path=\"url(#clipc04c10xyplot)\"><g class=\"barlayer mlayer\"><g class=\"trace bars\" shape-rendering=\"crispEdges\" style=\"opacity: 1;\"><g class=\"points\"><g class=\"point\"><path d=\"M149.44,332V164.64H164.39V332Z\" style=\"vector-effect: non-scaling-stroke; opacity: 1; stroke-width: 0px; fill: rgb(99, 110, 250); fill-opacity: 1;\"/></g><g class=\"point\"><path d=\"M164.39,332V16.6H179.33V332Z\" style=\"vector-effect: non-scaling-stroke; opacity: 1; stroke-width: 0px; fill: rgb(99, 110, 250); fill-opacity: 1;\"/></g><g class=\"point\"><path d=\"M179.33,332V119.59H194.28V332Z\" style=\"vector-effect: non-scaling-stroke; opacity: 1; stroke-width: 0px; fill: rgb(99, 110, 250); fill-opacity: 1;\"/></g><g class=\"point\"><path d=\"M194.28,332V106.71H209.22V332Z\" style=\"vector-effect: non-scaling-stroke; opacity: 1; stroke-width: 0px; fill: rgb(99, 110, 250); fill-opacity: 1;\"/></g><g class=\"point\"><path d=\"M209.22,332V113.15H224.17V332Z\" style=\"vector-effect: non-scaling-stroke; opacity: 1; stroke-width: 0px; fill: rgb(99, 110, 250); fill-opacity: 1;\"/></g><g class=\"point\"><path d=\"M224.17,332V183.96H239.11V332Z\" style=\"vector-effect: non-scaling-stroke; opacity: 1; stroke-width: 0px; fill: rgb(99, 110, 250); fill-opacity: 1;\"/></g><g class=\"point\"><path d=\"M239.11,332V158.21H254.06V332Z\" style=\"vector-effect: non-scaling-stroke; opacity: 1; stroke-width: 0px; fill: rgb(99, 110, 250); fill-opacity: 1;\"/></g><g class=\"point\"><path d=\"M254.06,332V248.32H269V332Z\" style=\"vector-effect: non-scaling-stroke; opacity: 1; stroke-width: 0px; fill: rgb(99, 110, 250); fill-opacity: 1;\"/></g><g class=\"point\"><path d=\"M269,332V209.7H283.94V332Z\" style=\"vector-effect: non-scaling-stroke; opacity: 1; stroke-width: 0px; fill: rgb(99, 110, 250); fill-opacity: 1;\"/></g><g class=\"point\"><path d=\"M283.94,332V222.58H298.89V332Z\" style=\"vector-effect: non-scaling-stroke; opacity: 1; stroke-width: 0px; fill: rgb(99, 110, 250); fill-opacity: 1;\"/></g><g class=\"point\"><path d=\"M298.89,332V254.76H313.83V332Z\" style=\"vector-effect: non-scaling-stroke; opacity: 1; stroke-width: 0px; fill: rgb(99, 110, 250); fill-opacity: 1;\"/></g><g class=\"point\"><path d=\"M313.83,332V254.76H328.78V332Z\" style=\"vector-effect: non-scaling-stroke; opacity: 1; stroke-width: 0px; fill: rgb(99, 110, 250); fill-opacity: 1;\"/></g><g class=\"point\"><path d=\"M328.78,332V293.38H343.72V332Z\" style=\"vector-effect: non-scaling-stroke; opacity: 1; stroke-width: 0px; fill: rgb(99, 110, 250); fill-opacity: 1;\"/></g><g class=\"point\"><path d=\"M343.72,332V299.82H358.67V332Z\" style=\"vector-effect: non-scaling-stroke; opacity: 1; stroke-width: 0px; fill: rgb(99, 110, 250); fill-opacity: 1;\"/></g><g class=\"point\"><path d=\"M358.67,332V312.69H373.61V332Z\" style=\"vector-effect: non-scaling-stroke; opacity: 1; stroke-width: 0px; fill: rgb(99, 110, 250); fill-opacity: 1;\"/></g><g class=\"point\"><path d=\"M373.61,332V299.82H388.56V332Z\" style=\"vector-effect: non-scaling-stroke; opacity: 1; stroke-width: 0px; fill: rgb(99, 110, 250); fill-opacity: 1;\"/></g><g class=\"point\"><path d=\"M0,0Z\" style=\"vector-effect: non-scaling-stroke; opacity: 1; stroke-width: 0px; fill: rgb(99, 110, 250); fill-opacity: 1;\"/></g><g class=\"point\"><path d=\"M403.5,332V319.13H418.44V332Z\" style=\"vector-effect: non-scaling-stroke; opacity: 1; stroke-width: 0px; fill: rgb(99, 110, 250); fill-opacity: 1;\"/></g><g class=\"point\"><path d=\"M418.44,332V325.56H433.39V332Z\" style=\"vector-effect: non-scaling-stroke; opacity: 1; stroke-width: 0px; fill: rgb(99, 110, 250); fill-opacity: 1;\"/></g><g class=\"point\"><path d=\"M433.39,332V325.56H448.33V332Z\" style=\"vector-effect: non-scaling-stroke; opacity: 1; stroke-width: 0px; fill: rgb(99, 110, 250); fill-opacity: 1;\"/></g><g class=\"point\"><path d=\"M448.33,332V325.56H463.28V332Z\" style=\"vector-effect: non-scaling-stroke; opacity: 1; stroke-width: 0px; fill: rgb(99, 110, 250); fill-opacity: 1;\"/></g><g class=\"point\"><path d=\"M0,0Z\" style=\"vector-effect: non-scaling-stroke; opacity: 1; stroke-width: 0px; fill: rgb(99, 110, 250); fill-opacity: 1;\"/></g><g class=\"point\"><path d=\"M0,0Z\" style=\"vector-effect: non-scaling-stroke; opacity: 1; stroke-width: 0px; fill: rgb(99, 110, 250); fill-opacity: 1;\"/></g><g class=\"point\"><path d=\"M0,0Z\" style=\"vector-effect: non-scaling-stroke; opacity: 1; stroke-width: 0px; fill: rgb(99, 110, 250); fill-opacity: 1;\"/></g><g class=\"point\"><path d=\"M0,0Z\" style=\"vector-effect: non-scaling-stroke; opacity: 1; stroke-width: 0px; fill: rgb(99, 110, 250); fill-opacity: 1;\"/></g><g class=\"point\"><path d=\"M523.06,332V325.56H538V332Z\" style=\"vector-effect: non-scaling-stroke; opacity: 1; stroke-width: 0px; fill: rgb(99, 110, 250); fill-opacity: 1;\"/></g></g></g><g class=\"trace bars\" shape-rendering=\"crispEdges\" style=\"opacity: 1;\"><g class=\"points\"><g class=\"point\"><path d=\"M0,332V319.13H14.94V332Z\" style=\"vector-effect: non-scaling-stroke; opacity: 1; stroke-width: 0px; fill: rgb(239, 85, 59); fill-opacity: 1;\"/></g><g class=\"point\"><path d=\"M14.94,332V203.27H29.89V332Z\" style=\"vector-effect: non-scaling-stroke; opacity: 1; stroke-width: 0px; fill: rgb(239, 85, 59); fill-opacity: 1;\"/></g><g class=\"point\"><path d=\"M29.89,332V203.27H44.83V332Z\" style=\"vector-effect: non-scaling-stroke; opacity: 1; stroke-width: 0px; fill: rgb(239, 85, 59); fill-opacity: 1;\"/></g><g class=\"point\"><path d=\"M44.83,332V132.46H59.78V332Z\" style=\"vector-effect: non-scaling-stroke; opacity: 1; stroke-width: 0px; fill: rgb(239, 85, 59); fill-opacity: 1;\"/></g><g class=\"point\"><path d=\"M59.78,332V164.64H74.72V332Z\" style=\"vector-effect: non-scaling-stroke; opacity: 1; stroke-width: 0px; fill: rgb(239, 85, 59); fill-opacity: 1;\"/></g><g class=\"point\"><path d=\"M74.72,332V61.66H89.67V332Z\" style=\"vector-effect: non-scaling-stroke; opacity: 1; stroke-width: 0px; fill: rgb(239, 85, 59); fill-opacity: 1;\"/></g><g class=\"point\"><path d=\"M89.67,332V42.35H104.61V332Z\" style=\"vector-effect: non-scaling-stroke; opacity: 1; stroke-width: 0px; fill: rgb(239, 85, 59); fill-opacity: 1;\"/></g><g class=\"point\"><path d=\"M104.61,332V48.78H119.56V332Z\" style=\"vector-effect: non-scaling-stroke; opacity: 1; stroke-width: 0px; fill: rgb(239, 85, 59); fill-opacity: 1;\"/></g><g class=\"point\"><path d=\"M119.56,332V23.04H134.5V332Z\" style=\"vector-effect: non-scaling-stroke; opacity: 1; stroke-width: 0px; fill: rgb(239, 85, 59); fill-opacity: 1;\"/></g><g class=\"point\"><path d=\"M134.5,332V23.04H149.44V332Z\" style=\"vector-effect: non-scaling-stroke; opacity: 1; stroke-width: 0px; fill: rgb(239, 85, 59); fill-opacity: 1;\"/></g><g class=\"point\"><path d=\"M149.44,164.64V23.04H164.39V164.64Z\" style=\"vector-effect: non-scaling-stroke; opacity: 1; stroke-width: 0px; fill: rgb(239, 85, 59); fill-opacity: 1;\"/></g></g></g></g></g><g class=\"overplot\"/><path class=\"xlines-above crisp\" d=\"M0,0\" style=\"fill: none;\"/><path class=\"ylines-above crisp\" d=\"M0,0\" style=\"fill: none;\"/><g class=\"overlines-above\"/><g class=\"xaxislayer-above\"><g class=\"xtick\"><text text-anchor=\"middle\" x=\"0\" y=\"445\" transform=\"translate(87.47,0)\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(42, 63, 95); fill-opacity: 1; white-space: pre; opacity: 1;\">0</text></g><g class=\"xtick\"><text text-anchor=\"middle\" x=\"0\" y=\"445\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(42, 63, 95); fill-opacity: 1; white-space: pre; opacity: 1;\" transform=\"translate(162.19,0)\">0.05</text></g><g class=\"xtick\"><text text-anchor=\"middle\" x=\"0\" y=\"445\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(42, 63, 95); fill-opacity: 1; white-space: pre; opacity: 1;\" transform=\"translate(236.92,0)\">0.1</text></g><g class=\"xtick\"><text text-anchor=\"middle\" x=\"0\" y=\"445\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(42, 63, 95); fill-opacity: 1; white-space: pre; opacity: 1;\" transform=\"translate(311.64,0)\">0.15</text></g><g class=\"xtick\"><text text-anchor=\"middle\" x=\"0\" y=\"445\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(42, 63, 95); fill-opacity: 1; white-space: pre; opacity: 1;\" transform=\"translate(386.36,0)\">0.2</text></g><g class=\"xtick\"><text text-anchor=\"middle\" x=\"0\" y=\"445\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(42, 63, 95); fill-opacity: 1; white-space: pre; opacity: 1;\" transform=\"translate(461.08,0)\">0.25</text></g><g class=\"xtick\"><text text-anchor=\"middle\" x=\"0\" y=\"445\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(42, 63, 95); fill-opacity: 1; white-space: pre; opacity: 1;\" transform=\"translate(535.81,0)\">0.3</text></g><g class=\"xtick\"><text text-anchor=\"middle\" x=\"0\" y=\"445\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(42, 63, 95); fill-opacity: 1; white-space: pre; opacity: 1;\" transform=\"translate(610.53,0)\">0.35</text></g></g><g class=\"yaxislayer-above\"><g class=\"ytick\"><text text-anchor=\"end\" x=\"79\" y=\"4.199999999999999\" transform=\"translate(0,432)\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(42, 63, 95); fill-opacity: 1; white-space: pre; opacity: 1;\">0</text></g><g class=\"ytick\"><text text-anchor=\"end\" x=\"79\" y=\"4.199999999999999\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(42, 63, 95); fill-opacity: 1; white-space: pre; opacity: 1;\" transform=\"translate(0,367.63)\">10</text></g><g class=\"ytick\"><text text-anchor=\"end\" x=\"79\" y=\"4.199999999999999\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(42, 63, 95); fill-opacity: 1; white-space: pre; opacity: 1;\" transform=\"translate(0,303.27)\">20</text></g><g class=\"ytick\"><text text-anchor=\"end\" x=\"79\" y=\"4.199999999999999\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(42, 63, 95); fill-opacity: 1; white-space: pre; opacity: 1;\" transform=\"translate(0,238.9)\">30</text></g><g class=\"ytick\"><text text-anchor=\"end\" x=\"79\" y=\"4.199999999999999\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(42, 63, 95); fill-opacity: 1; white-space: pre; opacity: 1;\" transform=\"translate(0,174.53)\">40</text></g><g class=\"ytick\"><text text-anchor=\"end\" x=\"79\" y=\"4.199999999999999\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(42, 63, 95); fill-opacity: 1; white-space: pre; opacity: 1;\" transform=\"translate(0,110.16)\">50</text></g></g><g class=\"overaxes-above\"/></g></g><g class=\"polarlayer\"/><g class=\"smithlayer\"/><g class=\"ternarylayer\"/><g class=\"geolayer\"/><g class=\"funnelarealayer\"/><g class=\"pielayer\"/><g class=\"iciclelayer\"/><g class=\"treemaplayer\"/><g class=\"sunburstlayer\"/><g class=\"glimages\"/><defs id=\"topdefs-c04c10\"><g class=\"clips\"/><clipPath id=\"legendc04c10\"><rect width=\"59\" height=\"67\" x=\"0\" y=\"0\"/></clipPath></defs><g class=\"layer-above\"><g class=\"imagelayer\"/><g class=\"shapelayer\"/></g><g class=\"infolayer\"><g class=\"legend\" pointer-events=\"all\" transform=\"translate(628.76,100)\"><rect class=\"bg\" shape-rendering=\"crispEdges\" style=\"stroke: rgb(68, 68, 68); stroke-opacity: 1; fill: rgb(255, 255, 255); fill-opacity: 1; stroke-width: 0px;\" width=\"59\" height=\"67\" x=\"0\" y=\"0\"/><g class=\"scrollbox\" transform=\"\" clip-path=\"url(#legendc04c10)\"><text class=\"legendtitletext\" text-anchor=\"start\" x=\"2\" y=\"18.2\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 14px; fill: rgb(42, 63, 95); fill-opacity: 1; white-space: pre;\">success</text><g class=\"groups\" transform=\"\"><g class=\"traces\" transform=\"translate(0,32.7)\" style=\"opacity: 1;\"><text class=\"legendtext\" text-anchor=\"start\" x=\"40\" y=\"4.680000000000001\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(42, 63, 95); fill-opacity: 1; white-space: pre;\">1</text><g class=\"layers\" style=\"opacity: 1;\"><g class=\"legendfill\"/><g class=\"legendlines\"/><g class=\"legendsymbols\"><g class=\"legendpoints\"><path class=\"legendundefined\" d=\"M6,6H-6V-6H6Z\" transform=\"translate(20,0)\" style=\"stroke-width: 0px; fill: rgb(99, 110, 250); fill-opacity: 1;\"/></g></g></g><rect class=\"legendtoggle\" x=\"0\" y=\"-9.5\" width=\"48\" height=\"19\" style=\"fill: rgb(0, 0, 0); fill-opacity: 0;\"/></g></g><g class=\"groups\" transform=\"\"><g class=\"traces\" transform=\"translate(0,51.7)\" style=\"opacity: 1;\"><text class=\"legendtext\" text-anchor=\"start\" x=\"40\" y=\"4.680000000000001\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(42, 63, 95); fill-opacity: 1; white-space: pre;\">0</text><g class=\"layers\" style=\"opacity: 1;\"><g class=\"legendfill\"/><g class=\"legendlines\"/><g class=\"legendsymbols\"><g class=\"legendpoints\"><path class=\"legendundefined\" d=\"M6,6H-6V-6H6Z\" transform=\"translate(20,0)\" style=\"stroke-width: 0px; fill: rgb(239, 85, 59); fill-opacity: 1;\"/></g></g></g><rect class=\"legendtoggle\" x=\"0\" y=\"-9.5\" width=\"48\" height=\"19\" style=\"fill: rgb(0, 0, 0); fill-opacity: 0;\"/></g></g></g><rect class=\"scrollbar\" rx=\"20\" ry=\"3\" width=\"0\" height=\"0\" style=\"fill: rgb(128, 139, 164); fill-opacity: 1;\" x=\"0\" y=\"0\"/></g><g class=\"g-gtitle\"><text class=\"gtitle\" x=\"35\" y=\"50\" text-anchor=\"start\" dy=\"0em\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 17px; fill: rgb(42, 63, 95); opacity: 1; font-weight: normal; white-space: pre;\"><tspan style=\"font-weight:bold\">Распределение лайков к просмотрам видео</tspan></text></g><g class=\"g-xtitle\"><text class=\"xtitle\" x=\"349\" y=\"472.8\" text-anchor=\"middle\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 14px; fill: rgb(42, 63, 95); opacity: 1; font-weight: normal; white-space: pre;\">ratio</text></g><g class=\"g-ytitle\"><text class=\"ytitle\" transform=\"rotate(-90,38.934375,266)\" x=\"38.934375\" y=\"266\" text-anchor=\"middle\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 14px; fill: rgb(42, 63, 95); opacity: 1; font-weight: normal; white-space: pre;\">count</text></g></g></svg>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = pd.DataFrame(\n",
    "    columns=['theme', 'likes', 'views', 'ratio', 'post_link', 'description', 'preview_path', 'video_path']\n",
    ")\n",
    "\n",
    "for json_file in os.listdir('../../data/jsons'):\n",
    "\n",
    "    if json_file.endswith('.json'):\n",
    "        posts_data = json.load(open(f\"{'../../data/jsons'}/{json_file}\", 'r'))\n",
    "        for post in posts_data:\n",
    "            if 'likes' in post and 'views' in post:\n",
    "                likes = convert_number(post['likes'])\n",
    "                views = convert_number(post['views'])\n",
    "                data.loc[data.shape[0] + 1] = [\n",
    "                    json_file.split('-')[1].split('.')[0],\n",
    "                    likes,\n",
    "                    views,\n",
    "                    likes / views,\n",
    "                    post['post_link'],\n",
    "                    post['description'],\n",
    "                    post['preview_path'],\n",
    "                    post['video_path'],\n",
    "                ]\n",
    "\n",
    "data = data[data.post_link.duplicated() == False].reset_index(drop=True)\n",
    "data['success'] = (data.ratio > 0.1) * 1\n",
    "fig_distr_lv = px.histogram(data, x='ratio', title='<b>Распределение лайков к просмотрам видео</b>', color='success')\n",
    "fig_distr_lv.show(renderer=renderer, height=512, widht=1024, autosize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>theme</th>\n",
       "      <th>likes</th>\n",
       "      <th>views</th>\n",
       "      <th>ratio</th>\n",
       "      <th>post_link</th>\n",
       "      <th>description</th>\n",
       "      <th>preview_path</th>\n",
       "      <th>video_path</th>\n",
       "      <th>success</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>memes</td>\n",
       "      <td>4099999</td>\n",
       "      <td>28900000</td>\n",
       "      <td>0.141868</td>\n",
       "      <td>https://www.tiktok.com/@felix_8099/video/70839...</td>\n",
       "      <td>#meme  #memes  #lore  #african  #africanlore  ...</td>\n",
       "      <td>../../data/previews/7083924753177677062.jpg</td>\n",
       "      <td>../../data/videos/video-7083924753177677062.mp4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506</th>\n",
       "      <td>memes</td>\n",
       "      <td>1400000</td>\n",
       "      <td>15500000</td>\n",
       "      <td>0.090323</td>\n",
       "      <td>https://www.tiktok.com/@area_of_meme/video/711...</td>\n",
       "      <td>TRY NOT TO LAUGH #funny  #funnyvideos  #fyp  #...</td>\n",
       "      <td>../../data/previews/7112823560283983110.jpg</td>\n",
       "      <td>../../data/videos/video-7112823560283983110.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>artificialintelligence</td>\n",
       "      <td>108500</td>\n",
       "      <td>1500000</td>\n",
       "      <td>0.072333</td>\n",
       "      <td>https://www.tiktok.com/@funx.arts/video/715256...</td>\n",
       "      <td>I couldn't believe what happened in the end! C...</td>\n",
       "      <td>../../data/previews/7152566906505989382.jpg</td>\n",
       "      <td>../../data/videos/video-7152566906505989382.mp4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>memes</td>\n",
       "      <td>5500000</td>\n",
       "      <td>40400000</td>\n",
       "      <td>0.136139</td>\n",
       "      <td>https://www.tiktok.com/@dsl1983/video/70889567...</td>\n",
       "      <td>#CapCut  #dsl1983  #lowquality  #lowqualitymem...</td>\n",
       "      <td>../../data/previews/7088956736110955782.jpg</td>\n",
       "      <td>../../data/videos/video-7088956736110955782.mp4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>learning</td>\n",
       "      <td>2200000</td>\n",
       "      <td>12200000</td>\n",
       "      <td>0.180328</td>\n",
       "      <td>https://www.tiktok.com/@billyvsco/video/706618...</td>\n",
       "      <td>Always a level of respect and privacy around h...</td>\n",
       "      <td>../../data/previews/7066182198269037870.jpg</td>\n",
       "      <td>../../data/videos/video-7066182198269037870.mp4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      theme    likes     views     ratio  \\\n",
       "496                   memes  4099999  28900000  0.141868   \n",
       "506                   memes  1400000  15500000  0.090323   \n",
       "65   artificialintelligence   108500   1500000  0.072333   \n",
       "500                   memes  5500000  40400000  0.136139   \n",
       "312                learning  2200000  12200000  0.180328   \n",
       "\n",
       "                                             post_link  \\\n",
       "496  https://www.tiktok.com/@felix_8099/video/70839...   \n",
       "506  https://www.tiktok.com/@area_of_meme/video/711...   \n",
       "65   https://www.tiktok.com/@funx.arts/video/715256...   \n",
       "500  https://www.tiktok.com/@dsl1983/video/70889567...   \n",
       "312  https://www.tiktok.com/@billyvsco/video/706618...   \n",
       "\n",
       "                                           description  \\\n",
       "496  #meme  #memes  #lore  #african  #africanlore  ...   \n",
       "506  TRY NOT TO LAUGH #funny  #funnyvideos  #fyp  #...   \n",
       "65   I couldn't believe what happened in the end! C...   \n",
       "500  #CapCut  #dsl1983  #lowquality  #lowqualitymem...   \n",
       "312  Always a level of respect and privacy around h...   \n",
       "\n",
       "                                    preview_path  \\\n",
       "496  ../../data/previews/7083924753177677062.jpg   \n",
       "506  ../../data/previews/7112823560283983110.jpg   \n",
       "65   ../../data/previews/7152566906505989382.jpg   \n",
       "500  ../../data/previews/7088956736110955782.jpg   \n",
       "312  ../../data/previews/7066182198269037870.jpg   \n",
       "\n",
       "                                          video_path  success  \n",
       "496  ../../data/videos/video-7083924753177677062.mp4        1  \n",
       "506  ../../data/videos/video-7112823560283983110.mp4        0  \n",
       "65   ../../data/videos/video-7152566906505989382.mp4        0  \n",
       "500  ../../data/videos/video-7088956736110955782.mp4        1  \n",
       "312  ../../data/videos/video-7066182198269037870.mp4        1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(5, random_state=21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Создание модели для выделения смыслов из кадров видео и превью"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модель MobileViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor = MobileViTFeatureExtractor.from_pretrained(\"apple/deeplabv3-mobilevit-small\")\n",
    "mobile_vit = MobileViTForSemanticSegmentation.from_pretrained(\"apple/deeplabv3-mobilevit-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дополнительные слои пулингов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VBarModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, _model: MobileViTForSemanticSegmentation):\n",
    "        super().__init__()\n",
    "        self.mobile_vit = _model\n",
    "        self.convs = torch.nn.Sequential(\n",
    "            torch.nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.mobile_vit(x).logits\n",
    "        x = self.convs(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 21, 16, 16])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vktrbr_model = VBarModel(mobile_vit).to(device)\n",
    "\n",
    "pic = ffmpegio.image.read(data.preview_path.iloc[0])\n",
    "pic = feature_extractor(images=pic, return_tensors='pt')['pixel_values']\n",
    "pic = pic.to(device)\n",
    "\n",
    "out_pic = vktrbr_model(pic).detach().to('cpu')\n",
    "out_pic_dim = out_pic.shape[1]\n",
    "out_pic.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Применение модели к каждому кадру модели\n",
    "\n",
    "## Тест на одном видео"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "FPS_DIV = 3  # Во сколько раз уменьшаем fps. Все видео в тиктоке 30 fps. По сути анализируем с fps = 10\n",
    "MAX_LENGTH = 90  # Фиксированное количество кадров. То есть обрезаем каждое видео до max_length / 30 * fps_mult секунд\n",
    "\n",
    "BATCH_SIZE = 4  # Количество кадров для обработки батчем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_video(path: str) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Читает видео и возвращает тензор с фичами\n",
    "    \"\"\"\n",
    "\n",
    "    _, video = ffmpegio.video.read(path, t=10.0)\n",
    "    video = video[::FPS_DIV][:MAX_LENGTH]\n",
    "\n",
    "    out_seg_video = []\n",
    "\n",
    "    for i in range(0, video.shape[0], BATCH_SIZE):\n",
    "        frames = [video[j] for j in range(i, min(i + BATCH_SIZE, video.shape[0]))]\n",
    "        frames = feature_extractor(images=frames, return_tensors='pt')['pixel_values']\n",
    "\n",
    "        out = vktrbr_model(frames.to(device)).detach().to('cpu')\n",
    "        out_seg_video.append(out)\n",
    "\n",
    "        del frames, out\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return torch.cat(out_seg_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([90, 21, 16, 16])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vid = read_video(data.video_path.sample(1, random_state=21).values[0])\n",
    "vid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Применяем ко всем видео"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "vid_torch_paths = []\n",
    "\n",
    "for idx, post in tqdm(data.iterrows()):\n",
    "    break\n",
    "    if os.path.exists(post.video_path.replace('mp4', 'th').replace('videos', 'videos-torch')):\n",
    "        continue\n",
    "\n",
    "    if os.path.exists(post.preview_path):\n",
    "        tensor_path = post.video_path.replace('mp4', 'th').replace('videos', 'videos-torch')\n",
    "\n",
    "        vid = read_video(post.video_path)\n",
    "        torch.save(vid, tensor_path)\n",
    "\n",
    "        del tensor_path\n",
    "        del vid\n",
    "\n",
    "        gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обработаем превью"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "vid_torch_paths = []\n",
    "\n",
    "for idx, post in tqdm(data.iterrows()):\n",
    "    break\n",
    "    if os.path.exists(post.preview_path.replace('jpg', 'th').replace('previews', 'previews-torch')):\n",
    "        continue\n",
    "\n",
    "    if os.path.exists(post.preview_path):\n",
    "        tensor_path = post.preview_path.replace('jpg', 'th').replace('previews', 'previews-torch')\n",
    "\n",
    "        preview = read_video(post.preview_path)\n",
    "        torch.save(preview, tensor_path)\n",
    "\n",
    "        del tensor_path\n",
    "        del preview\n",
    "\n",
    "        gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['torch_video_path'] = data.video_path.str.replace('mp4', 'th').str.replace('videos', 'videos-torch')\n",
    "data['torch_preview_path'] = data.preview_path.str.replace('jpg', 'th').str.replace('previews', 'previews-torch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Посты, обработанные успешно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[\n",
    "    data.apply(lambda x: os.path.exists(x.torch_video_path) and os.path.exists(x.torch_preview_path), axis='columns')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(653, 11)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Модель для понимания видео после обработки кадров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class VideoModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        p = 0.5\n",
    "        self.pic_cnn = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(21, 128, (2, 2), stride=2),\n",
    "            torch.nn.BatchNorm2d(128),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Conv2d(128, 256, (2, 2), stride=2),\n",
    "            torch.nn.BatchNorm2d(256),\n",
    "            torch.nn.Dropout2d(p),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Conv2d(256, 256, (4, 4), stride=2),\n",
    "            torch.nn.BatchNorm2d(256),\n",
    "            torch.nn.Dropout2d(p),\n",
    "            torch.nn.Flatten()\n",
    "        )\n",
    "\n",
    "        self.vid_cnn = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(21, 128, (2, 2), stride=2),\n",
    "            torch.nn.BatchNorm2d(128),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Conv2d(128, 256, (2, 2), stride=2),\n",
    "            torch.nn.BatchNorm2d(256),\n",
    "            torch.nn.Dropout2d(p),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Conv2d(256, 512, (2, 2), stride=2),\n",
    "            torch.nn.BatchNorm2d(512),\n",
    "            torch.nn.Dropout2d(p),\n",
    "            torch.nn.Flatten()\n",
    "        )\n",
    "\n",
    "        self.lstm = torch.nn.LSTM(2048, 256, 1, batch_first=True, bidirectional=True)\n",
    "        self.fc1 = torch.nn.Linear(256 * 2, 1024)\n",
    "        self.fc_norm = torch.nn.BatchNorm1d(256 * 2)\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "        self.fc2 = torch.nn.Linear(1024, 2)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        self.dropout = torch.nn.Dropout(p)\n",
    "\n",
    "        # xaiver init\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, torch.nn.Conv2d) or isinstance(m, torch.nn.Conv3d):\n",
    "                torch.nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    torch.nn.init.zeros_(m.bias)\n",
    "\n",
    "            elif isinstance(m, torch.nn.Linear):\n",
    "                torch.nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    torch.nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, video: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Использует превью как начальное скрытое состояние, а кадры видео как последовательность.\n",
    "        video[0] - превью, video[1] - видео\n",
    "\n",
    "        :param video: torch.Tensor, shape = (batch_size, frames + 1, 1344)\n",
    "        \"\"\"\n",
    "        _batch_size = video.shape[0]\n",
    "\n",
    "        _preview = video[:, 0, :, :]\n",
    "        _video = video[:, 1:, :, :]\n",
    "\n",
    "        h0 = self.pic_cnn(_preview).unsqueeze(0)\n",
    "        h0 = torch.nn.functional.pad(h0, (0, 0, 0, 0, 0, 1))\n",
    "        c0 = torch.zeros_like(h0)\n",
    "\n",
    "        _video = self.vid_cnn(_video.reshape(-1, 21, 16, 16))\n",
    "        _video = _video.reshape(_batch_size, 90, -1)\n",
    "\n",
    "        context, _ = self.lstm(_video, (h0, c0))\n",
    "        out = self.fc_norm(context[:, -1])\n",
    "        out = self.tanh(self.fc1(out))\n",
    "        out = self.dropout(out)\n",
    "        out = self.sigmoid(self.fc2(out))\n",
    "        return out\n",
    "\n",
    "\n",
    "class VideoPreprocessedDataset(Dataset):\n",
    "    def __init__(self, data: pd.DataFrame):\n",
    "        self.videos = [torch.load(data.torch_video_path.iloc[i]) for i in range(len(data))]\n",
    "        self.previews = [torch.load(data.torch_preview_path.iloc[i]) for i in range(len(data))]\n",
    "        self.target = data.success.values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.videos)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        out_video = self.videos[idx]\n",
    "        frames = out_video.shape[0]\n",
    "\n",
    "        # padding\n",
    "        out_video = torch.nn.functional.pad(out_video, (0, 0, 0, 0, 0, 0, MAX_LENGTH + 1 - frames, 0))\n",
    "\n",
    "        return out_video, torch.tensor(self.target[idx]).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, valid_dataset = train_test_split(data, test_size=0.5, random_state=21)\n",
    "train_dataset = VideoPreprocessedDataset(train_dataset)\n",
    "valid_dataset = VideoPreprocessedDataset(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, criterion, _device):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for batch_idx, (_data, target) in enumerate(train_loader):\n",
    "        _data, target = _data.to(device), target.to(_device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(_data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        # torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=1.0)\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    return train_loss\n",
    "\n",
    "\n",
    "def test(model, test_loader, _device):\n",
    "    # Accuracy\n",
    "    model.eval()\n",
    "    cnt = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (_data, target) in enumerate(test_loader):\n",
    "            _data, target = _data.to(device), target.to(_device)\n",
    "            output = model(_data)\n",
    "            cnt += torch.sum(output.argmax(dim=1) == target).item()\n",
    "\n",
    "    return cnt / len(test_loader.dataset)\n",
    "\n",
    "\n",
    "def train_model(_train_dataset, _valid_dataset, model, batch_size=32, epochs=10):\n",
    "    max_acc = 0\n",
    "    _device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(_device)\n",
    "    optimizer = torch.optim.RAdam(model.parameters(), weight_decay=0.01)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    train_loader = DataLoader(_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(_valid_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train(model, train_loader, optimizer, criterion, _device)\n",
    "\n",
    "        train_metric = test(model, train_loader, _device)\n",
    "        test_metric = test(model, test_loader, _device)\n",
    "\n",
    "        print(f\"Epoch: {epoch + 1:3d} \\t train loss: {train_loss:.4f} \"\n",
    "              f\"\\t train acc: {train_metric:.4f} \\t test acc: {test_metric:.4f}\")\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        if test_metric > max_acc:\n",
    "            max_acc = test_metric\n",
    "            torch.save(model, './best-model.th')\n",
    "\n",
    "    model = torch.load('./best-model.th')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:   1 \t train loss: 0.6803 \t train acc: 0.5552 \t test acc: 0.5076\n",
      "Epoch:   2 \t train loss: 0.7077 \t train acc: 0.6288 \t test acc: 0.5138\n",
      "Epoch:   3 \t train loss: 0.6815 \t train acc: 0.6871 \t test acc: 0.5810\n",
      "Epoch:   4 \t train loss: 0.6763 \t train acc: 0.7025 \t test acc: 0.5872\n",
      "Epoch:   5 \t train loss: 0.6653 \t train acc: 0.7270 \t test acc: 0.5902\n",
      "Epoch:   6 \t train loss: 0.6489 \t train acc: 0.7362 \t test acc: 0.5749\n",
      "Epoch:   7 \t train loss: 0.6279 \t train acc: 0.7546 \t test acc: 0.5749\n",
      "Epoch:   8 \t train loss: 0.6186 \t train acc: 0.7761 \t test acc: 0.5933\n",
      "Epoch:   9 \t train loss: 0.6015 \t train acc: 0.8037 \t test acc: 0.5841\n",
      "Epoch:  10 \t train loss: 0.6077 \t train acc: 0.8344 \t test acc: 0.5657\n"
     ]
    }
   ],
   "source": [
    "video_model = VideoModel()\n",
    "vm = train_model(train_dataset, valid_dataset, video_model, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_model = video_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = torch.zeros(len(valid_dataset), requires_grad=False)\n",
    "\n",
    "for i, (vid, target) in enumerate(DataLoader(valid_dataset, batch_size=1, shuffle=False)):\n",
    "    probs[i] = video_model(vid.to(device))[:, 1].detach().to('cpu')\n",
    "    vid.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nueramic_mathml.ml.metrics import auc_roc, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5999775068487544"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc_roc(\n",
    "    torch.tensor(valid_dataset.target),\n",
    "    probs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1999550136975088"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 * auc_roc(\n",
    "    torch.tensor(valid_dataset.target),\n",
    "    probs\n",
    ") - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5229358077049255"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(\n",
    "    torch.tensor(valid_dataset.target),\n",
    "    torch.ones_like(torch.tensor(valid_dataset.target)) * 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5932721495628357"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(\n",
    "    torch.tensor(valid_dataset.target),\n",
    "    (probs > 0.5) * 1\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
